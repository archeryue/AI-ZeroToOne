{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding: Language Model based on LSTM\n",
    "\n",
    " - Dataset: WikiText-2\n",
    " - Model: LSTM\n",
    " - Optimizer: Adam\n",
    " - Learning Rate: 0.001\n",
    " - Batch Size: 64\n",
    " - Epochs: 10\n",
    " - Dropout: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading WikiText-2 dataset from Hugging Face...\n",
      "Train dataset size: 36718\n",
      "Validation dataset size: 3760\n",
      "Test dataset size: 4358\n",
      "\n",
      "Looking for non-empty samples...\n",
      "Found non-empty sample at index 1\n",
      "First sample:  = Valkyria Chronicles III = \n",
      "\n",
      "\n",
      "Building vocabulary...\n",
      "Non-empty lines: 23767\n",
      "Total tokens: 1750345\n",
      "Vocabulary size: 28710\n",
      "Final vocabulary size (with special tokens): 28712\n",
      "Most common words: [('the', 130768), ('of', 57030), ('unk', 54625), ('and', 50735), ('in', 45015), ('to', 39521), ('a', 36523), ('was', 21008), ('on', 15140), ('as', 15058)]\n",
      "\n",
      "Sample tokenization:\n",
      "Original:  = Valkyria Chronicles III = \n",
      "\n",
      "Tokens: ['valkyria', 'chronicles', 'iii']\n",
      "Indices: [3767, 3830, 860]\n",
      "\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load WikiText-2 Dataset using Hugging Face datasets\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Load WikiText-2 dataset from Hugging Face\n",
    "print(\"Loading WikiText-2 dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
    "\n",
    "# Access the splits\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Show a sample\n",
    "# Find a non-empty sample\n",
    "print(f\"\\nLooking for non-empty samples...\")\n",
    "sample_text = None\n",
    "for i, example in enumerate(train_dataset):\n",
    "    if example['text'].strip():  # Find first non-empty line\n",
    "        sample_text = example['text']\n",
    "        print(f\"Found non-empty sample at index {i}\")\n",
    "        break\n",
    "print(f\"First sample: {sample_text[:200]}\")\n",
    "\n",
    "# Basic tokenization function\n",
    "def basic_tokenize(text):\n",
    "    # Convert to lowercase and split on whitespace and punctuation\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from training data\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "all_tokens = []\n",
    "non_empty_lines = 0\n",
    "for example in train_dataset:\n",
    "    if example['text'].strip():  # Skip empty lines\n",
    "        tokens = basic_tokenize(example['text'])\n",
    "        all_tokens.extend(tokens)\n",
    "        non_empty_lines += 1\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_counter = Counter(all_tokens)\n",
    "vocab_size = len(vocab_counter)\n",
    "print(f\"Non-empty lines: {non_empty_lines}\")\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create word to index mapping\n",
    "vocab = {'<unk>': 0, '<pad>': 1}\n",
    "for word, count in vocab_counter.most_common():\n",
    "    if word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "# Create index to word mapping\n",
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "print(f\"Final vocabulary size (with special tokens): {len(vocab)}\")\n",
    "print(f\"Most common words: {list(vocab_counter.most_common(10))}\")\n",
    "\n",
    "# Function to convert text to indices\n",
    "def text_to_indices(text, vocab):\n",
    "    tokens = basic_tokenize(text)\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# Convert a sample to see the tokenization\n",
    "sample_indices = text_to_indices(sample_text, vocab)\n",
    "print(f\"\\nSample tokenization:\")\n",
    "print(f\"Original: {sample_text[:100]}\")\n",
    "print(f\"Tokens: {basic_tokenize(sample_text)[:20]}\")\n",
    "print(f\"Indices: {sample_indices[:20]}\")\n",
    "\n",
    "print(\"\\nDataset loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "archer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
